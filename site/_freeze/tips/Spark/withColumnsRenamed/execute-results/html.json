{
  "hash": "a6253e1d03167155159e385f70179c86",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Rename Multiple Columns in PySpark with withColumnsRenamed()\"\ndescription: \"Renaming columns one at a time with .withColumnRenamed() in a loop stacks nested plan nodes and slows Spark's optimizer. Use .withColumnsRenamed() to apply all renames in a single pass with one clean plan node.\"\ndate-modified: \"02/19/2026\"\ndate-format: \"DD/MM/YYYY\"\ncategories: [pyspark, performance, Spark]\ntoc: true\ntoc-title: Navigation\ntags:\n  - databricks\n  - pyspark\n  - spark\n  - performance\n  - withColumnsRenamed\n  - dataframes\ndraft: false\nexecute:\n  enabled: true\n---\n\n\n::: {.callout-note title=\"Live Execution\" appearance=\"simple\"}\nAll code in this post is rendered against a Databricks workspace before publication.\n:::\n\n# Summary\n\n-   Calling `.withColumnRenamed()` in a loop has the same problem as `.withColumn()` loops: each call adds a new `Project` node to the logical plan\n-   `.withColumnsRenamed()` (Spark 3.4+ / DBR 13.0+) takes a dictionary of `{old_name: new_name}` and applies all renames in a single plan step\n-   Perfect for converting to snake_case, expanding abbreviations, removing special characters, and applying generic naming conventions across entire DataFrames\n\n# Introduction\n\nThis is the companion anti-pattern to [calling `.withColumn()` in a loop](withColumns.qmd). If you've ever ingested data from vendor feeds, legacy systems, APIs, or CSVs, you know the pain. Column names come in every flavour: `CustID`, `trnxAmt`, `ACCT-bal`, `pmt$Status`, `addr.Line1`. The instinct is to write a loop renaming them one by one.\n\nThe problem is identical to `.withColumn()` loops. Each `.withColumnRenamed()` call returns a new DataFrame with a new `Project` node stacked on the logical plan. With dozens of renames, the Catalyst optimizer bogs down traversing and optimizing through all those nested nodes.\n\n`.withColumnsRenamed()` was introduced in PySpark 3.4 to solve this. One dictionary mapping old names to new names, one plan node.\n\n```\ndf.withColumnsRenamed(colsMap: dict[str, str]) -> DataFrame\n```\n\n::: {.callout-note title=\"Setup: Create Sample Data\" collapse=\"true\"}\n\nEvery example below runs against this `df`, a realistic messy dataset with intentionally inconsistent column names.\n\nThe column names cover the usual mess: `CustID` (PascalCase), `custName` (camelCase), `trnx_date` (abbreviated snake_case), `trnxAmt` (camelCase + abbreviation), `ACCT-bal` (UPPER + hyphen), `pmt$Status` (dollar sign), `addr.Line1` (dot + PascalCase). This is what real source data looks like.\n\n::: {#fc4df480 .cell execution_count=2}\n``` {.python .cell-code}\nfrom faker import Faker\nfrom pyspark.sql.types import *\nimport random\n\nfake = Faker()\nFaker.seed(42)\nrandom.seed(42)\n\n# Generate 1000 rows with intentionally messy column names\nrows = []\nfor _ in range(1000):\n    rows.append((\n        fake.random_int(min=10000, max=99999),\n        fake.name(),\n        fake.date_between(\"-2y\", \"today\").isoformat(),\n        round(random.uniform(5.0, 9999.99), 4),\n        round(random.uniform(-500.0, 50000.0), 2),\n        random.choice([\"pending\", \"complete\", \"failed\", \"REVERSED\"]),\n        fake.street_address(),\n    ))\n\nschema = StructType([\n    StructField(\"CustID\",     IntegerType()),\n    StructField(\"custName\",   StringType()),\n    StructField(\"trnx_date\",  StringType()),\n    StructField(\"trnxAmt\",    DoubleType()),\n    StructField(\"ACCT-bal\",   DoubleType()),\n    StructField(\"pmt$Status\", StringType()),\n    StructField(\"addr.Line1\", StringType()),\n])\n\ndf = spark.createDataFrame(rows, schema)\nprint(df.columns)\ndf.show(5, truncate=False)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['CustID', 'custName', 'trnx_date', 'trnxAmt', 'ACCT-bal', 'pmt$Status', 'addr.Line1']\n+------+-----------------+----------+---------+--------+----------+------------------------------+\n|CustID|custName         |trnx_date |trnxAmt  |ACCT-bal|pmt$Status|addr.Line1                    |\n+------+-----------------+----------+---------+--------+----------+------------------------------+\n|93810 |Patrick Sanchez  |2024-06-01|6396.0645|763.04  |failed    |819 Johnson Course            |\n|38657 |Lance Hoffman    |2025-07-27|2452.6916|6546.67 |pending   |79402 Peterson Drives Apt. 511|\n|59797 |Ryan Munoz       |2025-10-02|6768.6046|44555.07|pending   |161 Calderon River Suite 931  |\n|16006 |Patricia Galloway|2025-11-06|5906.9668|1105.03 |pending   |4752 Kelly Skyway             |\n|44993 |Melinda Jones    |2025-05-29|2190.2844|25020.44|pending   |76483 Cameron Trail           |\n+------+-----------------+----------+---------+--------+----------+------------------------------+\nonly showing top 5 rows\n```\n:::\n:::\n\n\n:::\n\n# Code Examples\n\n### The Anti-Pattern\n\n::: {#8b3f3bb4 .cell execution_count=3}\n``` {.python .cell-code}\n# Each iteration creates a new DataFrame with a deeper logical plan\nrename_map = {\n    \"CustID\":     \"customer_id\",\n    \"trnxAmt\":    \"transaction_amount\",\n    \"ACCT-bal\":   \"account_balance\",\n    \"pmt$Status\": \"payment_status\",\n    \"addr.Line1\": \"address_line_1\",\n}\n\ndf_result = df\nfor old_name, new_name in rename_map.items():\n    df_result = df_result.withColumnRenamed(old_name, new_name)\n# 5 iterations = 5 nested Project nodes\n\nprint(df_result.columns)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['customer_id', 'custName', 'trnx_date', 'transaction_amount', 'account_balance', 'payment_status', 'address_line_1']\n```\n:::\n:::\n\n\n### The Fix: `.withColumnsRenamed()`\n\n::: {#a336c828 .cell execution_count=4}\n``` {.python .cell-code}\n# Single plan node, single pass, same result\ndf_result = df.withColumnsRenamed({\n    \"CustID\":     \"customer_id\",\n    \"trnxAmt\":    \"transaction_amount\",\n    \"ACCT-bal\":   \"account_balance\",\n    \"pmt$Status\": \"payment_status\",\n    \"addr.Line1\": \"address_line_1\",\n})\n\nprint(df_result.columns)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['customer_id', 'custName', 'trnx_date', 'transaction_amount', 'account_balance', 'payment_status', 'address_line_1']\n```\n:::\n:::\n\n\n### Real-World: Generic snake_case Conversion for All Columns\n\nWhen you want every column in your DataFrame to follow a consistent naming convention, build the rename dictionary dynamically:\n\n::: {#035b43f1 .cell execution_count=5}\n``` {.python .cell-code}\nimport re\n\ndef to_snake_case(name: str) -> str:\n    \"\"\"Convert any naming convention to clean snake_case.\"\"\"\n    name = re.sub(r'[.$\\-@#!&*()+=/\\\\]', '_', name)      # special chars to _\n    name = re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', name)   # camelCase split\n    name = re.sub(r'([A-Z]+)([A-Z][a-z])', r'\\1_\\2', name) # ACRONYMWord split\n    name = re.sub(r'_{2,}', '_', name)                      # collapse doubles\n    return name.lower().strip('_')\n\n# Build rename dict from current column names\nrename_map = {col: to_snake_case(col) for col in df.columns}\nprint(rename_map)\n\n# Apply all renames in one pass\ndf_clean = df.withColumnsRenamed(rename_map)\nprint(df_clean.columns)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'CustID': 'cust_id', 'custName': 'cust_name', 'trnx_date': 'trnx_date', 'trnxAmt': 'trnx_amt', 'ACCT-bal': 'acct_bal', 'pmt$Status': 'pmt_status', 'addr.Line1': 'addr_line1'}\n['cust_id', 'cust_name', 'trnx_date', 'trnx_amt', 'acct_bal', 'pmt_status', 'addr_line1']\n```\n:::\n:::\n\n\n### Real-World: Expand Common Abbreviations\n\nData teams often inherit column names full of cryptic abbreviations from source systems. Combine snake_case conversion with abbreviation expansion for truly readable schemas:\n\n::: {#810aa699 .cell execution_count=6}\n``` {.python .cell-code}\nimport re\n\nABBREVIATIONS = {\n    \"trnx\": \"transaction\", \"cust\": \"customer\",\n    \"acct\": \"account\",     \"pmt\":  \"payment\",\n    \"addr\": \"address\",     \"amt\":  \"amount\",\n    \"bal\":  \"balance\",     \"qty\":  \"quantity\",\n    \"desc\": \"description\", \"dt\":   \"date\",\n    \"ts\":   \"timestamp\",   \"nbr\":  \"number\",\n    \"num\":  \"number\",      \"cd\":   \"code\",\n    \"nm\":   \"name\",\n}\n\ndef expand_column_name(name: str) -> str:\n    \"\"\"snake_case + expand common abbreviations.\"\"\"\n    # First convert to snake_case\n    snake = re.sub(r'[.$\\-@#!]', '_', name)\n    snake = re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', snake)\n    snake = re.sub(r'_{2,}', '_', snake).lower().strip('_')\n    # Then expand abbreviations\n    parts = snake.split(\"_\")\n    return \"_\".join(ABBREVIATIONS.get(p, p) for p in parts)\n\nrename_map = {col: expand_column_name(col) for col in df.columns}\nprint(rename_map)\n\ndf_clean = df.withColumnsRenamed(rename_map)\nprint(df_clean.columns)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'CustID': 'customer_id', 'custName': 'customer_name', 'trnx_date': 'transaction_date', 'trnxAmt': 'transaction_amount', 'ACCT-bal': 'account_balance', 'pmt$Status': 'payment_status', 'addr.Line1': 'address_line1'}\n['customer_id', 'customer_name', 'transaction_date', 'transaction_amount', 'account_balance', 'payment_status', 'address_line1']\n```\n:::\n:::\n\n\n### Real-World: Strip Vendor Prefixes\n\nVendor data often arrives with prefixes like `SRC_`, `RAW_`, or `STG_`. Here we create a prefixed version and strip it back:\n\n::: {#13f97ec2 .cell execution_count=7}\n``` {.python .cell-code}\n# First simulate vendor-prefixed data from our setup df\ndf_vendor = df.withColumnsRenamed({col: f\"SRC_{col}\" for col in df.columns})\nprint(\"Before:\", df_vendor.columns)\n\n# Now strip the prefix in one pass\nprefix = \"SRC_\"\nrename_map = {\n    col: col[len(prefix):] if col.startswith(prefix) else col\n    for col in df_vendor.columns\n}\ndf_clean = df_vendor.withColumnsRenamed(rename_map)\nprint(\"After: \", df_clean.columns)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBefore: ['SRC_CustID', 'SRC_custName', 'SRC_trnx_date', 'SRC_trnxAmt', 'SRC_ACCT-bal', 'SRC_pmt$Status', 'SRC_addr.Line1']\nAfter:  ['CustID', 'custName', 'trnx_date', 'trnxAmt', 'ACCT-bal', 'pmt$Status', 'addr.Line1']\n```\n:::\n:::\n\n\n### Scala Equivalent\n\n``` scala\nimport org.apache.spark.sql.functions._\n\n// Scala uses a Map of old name to new name\nval dfResult = df.withColumnsRenamed(Map(\n  \"CustID\"     -> \"customer_id\",\n  \"trnxAmt\"    -> \"transaction_amount\",\n  \"ACCT-bal\"   -> \"account_balance\",\n  \"pmt$Status\" -> \"payment_status\",\n  \"addr.Line1\" -> \"address_line_1\"\n))\n```\n\n::: {.callout-tip title=\"Check Your Spark Version\" appearance=\"simple\"}\n`.withColumnsRenamed()` requires PySpark 3.4+ (Databricks Runtime 13.0+).\n:::\n\n# Detail\n\n## The Same Problem, Different Method\n\n`.withColumnRenamed()` suffers from exactly the same plan-nesting issue as `.withColumn()`. Each call returns a new DataFrame with a new `Project` node appended to the logical plan. See [the companion tip on .withColumns()](withColumns.qmd) for the full Catalyst optimizer breakdown.\n\nThe key difference is that `.withColumnsRenamed()` works with strings only, no `Column` expressions needed, making it even simpler to use.\n\n## Common Rename Patterns\n\n-   **snake_case conversion**: Teams following Python naming conventions\n-   **Abbreviation expansion**: Improved readability and self-documenting schemas\n-   **Prefix/suffix stripping**: Vendor data normalisation\n-   **Special character removal**: Compatibility with downstream systems (Parquet, Delta, BI tools that choke on dots, dollar signs, or hyphens in column names)\n\n# References & Further Reading\n\n-   [PySpark DataFrame.withColumnsRenamed API Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumnsRenamed.html)\n-   [Companion Tip: From 20 withColumn() Calls to 1 withColumns()](withColumns.qmd)\n\n",
    "supporting": [
      "withColumnsRenamed_files"
    ],
    "filters": [],
    "includes": {}
  }
}