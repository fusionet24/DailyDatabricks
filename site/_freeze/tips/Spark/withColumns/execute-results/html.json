{
  "hash": "2ae9e75d29fcdc6fb97f69e4dcaf223d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"From 20 withColumn() Calls to 1 withColumns()\"\ndescription: \"Calling .withColumn() in a loop creates a new DataFrame per iteration, bloating your logical plan and slowing down the Catalyst optimizer. Use .withColumns() to apply all transformations in a single pass.\"\ndate-modified: \"02/18/2026\"\ndate-format: \"DD/MM/YYYY\"\ncategories: [pyspark, performance, Spark]\ntoc: true\ntoc-title: Navigation\ntags:\n  - databricks\n  - pyspark\n  - spark\n  - performance\n  - withColumns\n  - dataframes\ndraft: false\nexecute:\n  enabled: true\n---\n\n\n::: {.callout-note title=\"Live Execution\" appearance=\"simple\"}\nAll code in this post is rendered against a Databricks workspace before publication.\n:::\n\n# Summary\n\n-   Calling `.withColumn()` in a loop creates a new DataFrame per iteration, generating deeply nested logical plans that slow down the Catalyst optimizer\n-   `.withColumns()` (Spark 3.3+ / DBR 12.0+) accepts a dictionary of `{column_name: expression}` and applies all transformations in a single plan step\n-   Real-world use cases: metadata enrichment, column standardisation, acronym expansion, cleaning messy column names\n\n# Introduction\n\nOne of the most common PySpark anti-patterns is wrapping `.withColumn()` in a `for` loop. It reads well, feels Pythonic, and works... until it doesn't.\n\nEvery call to `.withColumn()` returns a **brand-new DataFrame** object. Spark stacks each transformation as a separate `Project` node in the logical plan. With 50 columns in your loop, that's 50 nested `Project` nodes the Catalyst optimizer has to traverse, analyze, and optimize through. With hundreds of columns (common in wide fact tables or IoT data) you can even hit a `StackOverflowError` because plan tree traversal is recursive.\n\n`.withColumns()` was introduced in PySpark 3.3 to solve exactly this. One dictionary in, one plan node out.\n\n```\ndf.withColumns(colsMap: dict[str, Column]) -> DataFrame\n```\n\n::: {.callout-note title=\"Setup: Create Sample Data\" collapse=\"true\"}\n\nEvery example below runs against this `df`, a realistic messy dataset that mirrors what you'd typically get from a legacy source system.\n\nThe column names are intentionally inconsistent: `CustID` (PascalCase), `custName` (camelCase), `trnx_date` (abbreviated snake_case), `trnxAmt` (camelCase + abbreviation), `ACCT-bal` (UPPER + hyphen), `pmt$Status` (dollar sign), `addr.Line1` (dot + PascalCase). This is what real source data can look like.\n\n::: {#38fd5f57 .cell execution_count=2}\n``` {.python .cell-code}\nfrom faker import Faker\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import *\nimport random\n\nfake = Faker()\nFaker.seed(42)\nrandom.seed(42)\n\n# Generate 1000 rows of realistic messy data\nrows = []\nfor _ in range(1000):\n    rows.append((\n        fake.random_int(min=10000, max=99999),          # CustID\n        f\"  {fake.name()}  \",                            # custName (with whitespace)\n        fake.date_between(\"-2y\", \"today\").isoformat(),   # trnx_date\n        round(random.uniform(5.0, 9999.99), 4),         # trnxAmt (extra decimals)\n        round(random.uniform(-500.0, 50000.0), 2),      # ACCT-bal\n        random.choice([\"pending\", \"complete\", \"failed\", \"REVERSED\"]),  # pmt$Status\n        fake.street_address(),                           # addr.Line1\n    ))\n\nschema = StructType([\n    StructField(\"CustID\",     IntegerType()),\n    StructField(\"custName\",   StringType()),\n    StructField(\"trnx_date\",  StringType()),\n    StructField(\"trnxAmt\",    DoubleType()),\n    StructField(\"ACCT-bal\",   DoubleType()),\n    StructField(\"pmt$Status\", StringType()),\n    StructField(\"addr.Line1\", StringType()),\n])\n\ndf = spark.createDataFrame(rows, schema)\ndf.show(5, truncate=False)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+------+---------------------+----------+---------+--------+----------+------------------------------+\n|CustID|custName             |trnx_date |trnxAmt  |ACCT-bal|pmt$Status|addr.Line1                    |\n+------+---------------------+----------+---------+--------+----------+------------------------------+\n|93810 |  Patrick Sanchez    |2024-05-30|6396.0645|763.04  |failed    |819 Johnson Course            |\n|38657 |  Lance Hoffman      |2025-07-25|2452.6916|6546.67 |pending   |79402 Peterson Drives Apt. 511|\n|59797 |  Ryan Munoz         |2025-09-30|6768.6046|44555.07|pending   |161 Calderon River Suite 931  |\n|16006 |  Patricia Galloway  |2025-11-04|5906.9668|1105.03 |pending   |4752 Kelly Skyway             |\n|44993 |  Melinda Jones      |2025-05-27|2190.2844|25020.44|pending   |76483 Cameron Trail           |\n+------+---------------------+----------+---------+--------+----------+------------------------------+\nonly showing top 5 rows\n```\n:::\n:::\n\n\n:::\n\n# Code Examples\n\n### The Anti-Pattern\n\n::: {#dd333d28 .cell execution_count=3}\n``` {.python .cell-code}\nfrom pyspark.sql import functions as F\n\n# Each iteration creates a new DataFrame with a deeper logical plan\ncolumns_to_add = {\n    \"trnx_date_upper\":   F.upper(F.col(\"trnx_date\")),\n    \"cust_name_trimmed\":  F.trim(F.col(\"custName\")),\n    \"acct_balance_abs\":   F.abs(F.col(\"`ACCT-bal`\")),\n    \"txn_amt_rounded\":    F.round(F.col(\"trnxAmt\"), 2),\n    \"load_timestamp\":     F.current_timestamp(),\n}\n\ndf_result = df\nfor col_name, expression in columns_to_add.items():\n    df_result = df_result.withColumn(col_name, expression)\n# 5 iterations = 5 nested Project nodes in the plan\n\ndf_result.show(5, truncate=False)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+------+---------------------+----------+---------+--------+----------+------------------------------+---------------+-----------------+----------------+---------------+--------------------------+\n|CustID|custName             |trnx_date |trnxAmt  |ACCT-bal|pmt$Status|addr.Line1                    |trnx_date_upper|cust_name_trimmed|acct_balance_abs|txn_amt_rounded|load_timestamp            |\n+------+---------------------+----------+---------+--------+----------+------------------------------+---------------+-----------------+----------------+---------------+--------------------------+\n|93810 |  Patrick Sanchez    |2024-05-30|6396.0645|763.04  |failed    |819 Johnson Course            |2024-05-30     |Patrick Sanchez  |763.04          |6396.06        |2026-02-18 12:13:29.416136|\n|38657 |  Lance Hoffman      |2025-07-25|2452.6916|6546.67 |pending   |79402 Peterson Drives Apt. 511|2025-07-25     |Lance Hoffman    |6546.67         |2452.69        |2026-02-18 12:13:29.416136|\n|59797 |  Ryan Munoz         |2025-09-30|6768.6046|44555.07|pending   |161 Calderon River Suite 931  |2025-09-30     |Ryan Munoz       |44555.07        |6768.6         |2026-02-18 12:13:29.416136|\n|16006 |  Patricia Galloway  |2025-11-04|5906.9668|1105.03 |pending   |4752 Kelly Skyway             |2025-11-04     |Patricia Galloway|1105.03         |5906.97        |2026-02-18 12:13:29.416136|\n|44993 |  Melinda Jones      |2025-05-27|2190.2844|25020.44|pending   |76483 Cameron Trail           |2025-05-27     |Melinda Jones    |25020.44        |2190.28        |2026-02-18 12:13:29.416136|\n+------+---------------------+----------+---------+--------+----------+------------------------------+---------------+-----------------+----------------+---------------+--------------------------+\nonly showing top 5 rows\n```\n:::\n:::\n\n\n### The Fix: `.withColumns()`\n\n::: {#29f6ac31 .cell execution_count=4}\n``` {.python .cell-code}\nfrom pyspark.sql import functions as F\n\n# Single plan node, single pass, same result\ndf_result = df.withColumns({\n    \"trnx_date_upper\":   F.upper(F.col(\"trnx_date\")),\n    \"cust_name_trimmed\":  F.trim(F.col(\"custName\")),\n    \"acct_balance_abs\":   F.abs(F.col(\"`ACCT-bal`\")),\n    \"txn_amt_rounded\":    F.round(F.col(\"trnxAmt\"), 2),\n    \"load_timestamp\":     F.current_timestamp(),\n})\n\ndf_result.show(5, truncate=False)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+------+---------------------+----------+---------+--------+----------+------------------------------+---------------+-----------------+----------------+---------------+--------------------------+\n|CustID|custName             |trnx_date |trnxAmt  |ACCT-bal|pmt$Status|addr.Line1                    |trnx_date_upper|cust_name_trimmed|acct_balance_abs|txn_amt_rounded|load_timestamp            |\n+------+---------------------+----------+---------+--------+----------+------------------------------+---------------+-----------------+----------------+---------------+--------------------------+\n|93810 |  Patrick Sanchez    |2024-05-30|6396.0645|763.04  |failed    |819 Johnson Course            |2024-05-30     |Patrick Sanchez  |763.04          |6396.06        |2026-02-18 12:13:31.643766|\n|38657 |  Lance Hoffman      |2025-07-25|2452.6916|6546.67 |pending   |79402 Peterson Drives Apt. 511|2025-07-25     |Lance Hoffman    |6546.67         |2452.69        |2026-02-18 12:13:31.643766|\n|59797 |  Ryan Munoz         |2025-09-30|6768.6046|44555.07|pending   |161 Calderon River Suite 931  |2025-09-30     |Ryan Munoz       |44555.07        |6768.6         |2026-02-18 12:13:31.643766|\n|16006 |  Patricia Galloway  |2025-11-04|5906.9668|1105.03 |pending   |4752 Kelly Skyway             |2025-11-04     |Patricia Galloway|1105.03         |5906.97        |2026-02-18 12:13:31.643766|\n|44993 |  Melinda Jones      |2025-05-27|2190.2844|25020.44|pending   |76483 Cameron Trail           |2025-05-27     |Melinda Jones    |25020.44        |2190.28        |2026-02-18 12:13:31.643766|\n+------+---------------------+----------+---------+--------+----------+------------------------------+---------------+-----------------+----------------+---------------+--------------------------+\nonly showing top 5 rows\n```\n:::\n:::\n\n\n### Real-World: Standardise Messy Columns and Expand Abbreviations\n\nOur setup DataFrame has columns like `CustID`, `trnxAmt`, `ACCT-bal`, `pmt$Status`, `addr.Line1`. Mixed casing, special characters, and cryptic abbreviations everywhere. Here's how to clean them all in one pass:\n\n::: {#d2e0b83e .cell execution_count=5}\n``` {.python .cell-code}\nfrom pyspark.sql import functions as F\nimport re\n\ndef to_snake_case(name: str) -> str:\n    \"\"\"Convert any naming convention to snake_case.\"\"\"\n    name = re.sub(r'[.$\\-@#!]', '_', name)            # special chars to _\n    name = re.sub(r'([a-z])([A-Z])', r'\\1_\\2', name)  # camelCase split\n    name = re.sub(r'_{2,}', '_', name)                 # collapse doubles\n    return name.lower().strip('_')\n\nABBREVIATIONS = {\n    \"trnx\": \"transaction\", \"cust\": \"customer\",\n    \"acct\": \"account\",     \"pmt\":  \"payment\",\n    \"addr\": \"address\",     \"amt\":  \"amount\",\n    \"bal\":  \"balance\",     \"qty\":  \"quantity\",\n    \"desc\": \"description\", \"dt\":   \"date\",\n    \"nbr\":  \"number\",      \"cd\":   \"code\",\n    \"nm\":   \"name\",\n}\n\ndef expand_abbreviations(name: str) -> str:\n    \"\"\"Expand common data abbreviations in a column name.\"\"\"\n    parts = name.split(\"_\")\n    return \"_\".join(ABBREVIATIONS.get(p, p) for p in parts)\n\n# Build the transformation dict dynamically\ntransformations = {}\nfor col_name in df.columns:\n    clean_name = expand_abbreviations(to_snake_case(col_name))\n    transformations[clean_name] = F.col(f\"`{col_name}`\")\n\ndf_clean = df.withColumns(transformations)\nprint(df_clean.columns)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['CustID', 'custName', 'trnx_date', 'trnxAmt', 'ACCT-bal', 'pmt$Status', 'addr.Line1', 'customer_id', 'customer_name', 'transaction_date', 'transaction_amount', 'account_balance', 'payment_status', 'address_line1']\n```\n:::\n:::\n\n\n### Real-World: Metadata Enrichment in One Pass\n\nAdding audit columns is one of the most common ETL tasks. Do it in one call instead of five:\n\n::: {#c6e2bbb8 .cell execution_count=6}\n``` {.python .cell-code}\nfrom pyspark.sql import functions as F\n\npipeline_run_id = \"run-2026-02-16-001\"\n\ndf_enriched = df.withColumns({\n    \"ingestion_timestamp\": F.current_timestamp(),\n    \"source_system\":       F.lit(\"CRM_PROD\"),\n    \"pipeline_run_id\":     F.lit(pipeline_run_id),\n    \"row_hash\":            F.sha2(F.concat_ws(\"|\", *[F.col(f\"`{c}`\").cast(\"string\") for c in df.columns]), 256),\n    \"is_valid\": F.when(\n        F.col(\"trnxAmt\").isNotNull() & (F.col(\"trnxAmt\") > 0), True\n    ).otherwise(False),\n})\n\ndf_enriched.select(\"CustID\", \"ingestion_timestamp\", \"source_system\", \"row_hash\", \"is_valid\").show(5, truncate=40)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+------+--------------------------+-------------+----------------------------------------+--------+\n|CustID|       ingestion_timestamp|source_system|                                row_hash|is_valid|\n+------+--------------------------+-------------+----------------------------------------+--------+\n| 93810|2026-02-18 12:13:32.732663|     CRM_PROD|0a16938d05f57ecdac3a93290a6b091113435...|    true|\n| 38657|2026-02-18 12:13:32.732663|     CRM_PROD|2852cc60ce03fa94327046eaa23647536574e...|    true|\n| 59797|2026-02-18 12:13:32.732663|     CRM_PROD|8fdc5170c3c067b621370b9b015f84dd4e66c...|    true|\n| 16006|2026-02-18 12:13:32.732663|     CRM_PROD|15e0ed67555e8ed6a965c5fa76a6071d3b953...|    true|\n| 44993|2026-02-18 12:13:32.732663|     CRM_PROD|7dcd0fb0db5ffe761939d1de92d8b1798783e...|    true|\n+------+--------------------------+-------------+----------------------------------------+--------+\nonly showing top 5 rows\n```\n:::\n:::\n\n\n### Scala Equivalent\n\n``` scala\nimport org.apache.spark.sql.functions._\n\n// Scala uses a Map of column name to expression\nval dfResult = df.withColumns(Map(\n  \"trnx_date_upper\"   -> upper(col(\"trnx_date\")),\n  \"cust_name_trimmed\"  -> trim(col(\"custName\")),\n  \"acct_balance_abs\"   -> abs(col(\"`ACCT-bal`\")),\n  \"txn_amt_rounded\"    -> round(col(\"trnxAmt\"), 2),\n  \"load_timestamp\"     -> current_timestamp()\n))\n```\n\n::: {.callout-tip title=\"Check Your Spark Version\" appearance=\"simple\"}\n`.withColumns()` requires PySpark 3.3+. If you're not using that version yet upgrade your databricks runtime for goodness sake.\n:::\n\n# Detail\n\n## Why `.withColumn()` in a Loop Is Costly\n\nEach `.withColumn()` call returns a new `DataFrame` with a new `Project` node appended to the unresolved logical plan. With N loop iterations, you get N nested `Project` nodes.\n\nThe Catalyst optimizer has to traverse and optimize through each one. Analysis, resolution, and optimization passes all scale with plan depth. Beyond roughly 100 columns you can hit `StackOverflowError` because plan tree traversal is recursive. Even before that, planning time can balloon from milliseconds to seconds.\n\n## How `.withColumns()` Fixes This\n\n1.  **Single Plan Node**: All column expressions merge into one `Project` node\n2.  **Faster Optimization**: Catalyst processes one flat set of expressions instead of a deeply nested tree\n3.  **Cleaner Code**: A dictionary is more readable and maintainable than a loop\n4.  **Prevents StackOverflow**: Even with hundreds of columns, the plan stays shallow\n5.  **Idiomatic**: This is the pattern the Spark developers intended for multi-column operations\n\n## When to Use `.withColumns()`\n\n-   Adding metadata or audit columns to ETL outputs\n-   Applying type casts or formatting to multiple columns\n-   Building derived columns from business rules\n-   Any time you find yourself writing a loop with `.withColumn()`\n\n# References & Further Reading\n\n-   [PySpark DataFrame.withColumns API Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumns.html)\n-   [Companion Tip: Rename Multiple Columns in PySpark with withColumnsRenamed()](withColumnsRenamed.qmd)\n\n",
    "supporting": [
      "withColumns_files"
    ],
    "filters": [],
    "includes": {}
  }
}