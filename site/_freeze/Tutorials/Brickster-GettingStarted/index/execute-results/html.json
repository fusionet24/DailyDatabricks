{
  "hash": "c11345a610a3398501bb067f3e94aadd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Getting Started with Brickster: R for Databricks\"\nsubtitle: \"Connect to Databricks from R using the brickster package\"\ndescription: \"Learn how to use the brickster R package to connect to Databricks, explore Unity Catalog, run SQL queries, and analyse data with dplyr.\"\ndate-modified: \"02/18/2026\"\ndate-format: \"DD/MM/YYYY\"\ncategories: [brickster, R, Tutorials]\ntoc: true\ntoc-title: Navigation\ntags:\n  - databricks\n  - brickster\n  - R\n  - dplyr\n  - unity-catalog\n  - DBI\ndraft: false\nexecute:\n  enabled: true\n---\n\n::: {.callout-note title=\"Live Execution\" appearance=\"simple\"}\nAll the code in this tutorial is rendered against a Databricks workspace before publication.\n:::\n\n::: {.callout-note title=\"connect.R — Project Helper\" collapse=\"true\"}\nThis tutorial uses a shared helper at `_databricks/connect.R` that loads credentials from `.env` and auto-discovers your SQL Warehouse. Here's the full source:\n\n```r\nlibrary(brickster)\nlibrary(DBI)\n\n# Load .env file if it exists (mirrors Python's python-dotenv behaviour)\n# Walk up from working directory to find .env\nfind_env_file <- function() {\n  dir <- getwd()\n  while (dir != dirname(dir)) {\n    candidate <- file.path(dir, \".env\")\n    if (file.exists(candidate)) return(candidate)\n    dir <- dirname(dir)\n  }\n  NULL\n}\n\nenv_file <- find_env_file()\nif (!is.null(env_file)) {\n  readRenviron(env_file)\n}\n\n#' Get a DBI connection to Databricks via brickster\n#'\n#' Reads DATABRICKS_HOST and DATABRICKS_TOKEN from environment variables.\n#' If DATABRICKS_WAREHOUSE_ID is set, uses it directly. Otherwise,\n#' auto-discovers the first available SQL warehouse.\n#'\n#' @param warehouse_id SQL Warehouse ID (optional — auto-discovers if not set)\n#' @return A DBI connection object\nget_databricks_connection <- function(warehouse_id = Sys.getenv(\"DATABRICKS_WAREHOUSE_ID\")) {\n  if (nchar(warehouse_id) == 0) {\n    message(\"DATABRICKS_WAREHOUSE_ID not set, discovering warehouses...\")\n    warehouses <- db_sql_warehouse_list()\n    if (length(warehouses) == 0) {\n      stop(\"No SQL Warehouses found. Create one in the Databricks UI.\")\n    }\n    warehouse_id <- warehouses[[1]]$id\n    message(\"Using warehouse: \", warehouses[[1]]$name, \" (\", warehouse_id, \")\")\n  }\n  dbConnect(\n    DatabricksSQL(),\n    warehouse_id = warehouse_id\n  )\n}\n```\n:::\n\n## Summary\n\n-   **Brickster** is an R package from Databricks Labs that provides native R integration with Databricks — no ODBC driver required\n-   You'll install brickster, connect to a Databricks workspace, and explore Unity Catalog (catalogs, schemas, tables)\n-   We'll install dplyr on a Databricks cluster, run SQL queries, and use dplyr verbs to analyse the **Bakehouse** sample dataset\n-   Finally, we'll show how you could write results back to a new Delta table\n\n## What is Brickster?\n\n[Brickster](https://databrickslabs.github.io/brickster/) is a Databricks Labs R package that gives R users first-class access to Databricks workspaces. Where Python users have `databricks-connect`, R users have brickster.\n\nKey features:\n\n-   **DBI backend** — Standard R database connectivity via `dbConnect(DatabricksSQL(), ...)`\n-   **dbplyr integration** — Write dplyr code that translates to Databricks SQL automatically\n-   **Unity Catalog API** — Browse and manage catalogs, schemas, tables, and volumes programmatically\n-   **Cluster management** — Start, stop, and install libraries on clusters from R\n-   **No ODBC required** — Connects directly via the Databricks SQL Statement Execution API\n-   **RStudio integration** — Browse workspace resources via the Connections Pane with `open_workspace()`\n\n## Prerequisites\n\nBefore you begin, make sure you have:\n\n-   **R >= 4.1.0** installed\n-   A **Databricks workspace** with a SQL Warehouse available\n-   A **Personal Access Token** (or OAuth configured) for authentication\n-   Access to the **Bakehouse sample dataset** in Unity Catalog (available by default in most workspaces)\n\nInstall brickster from CRAN:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"brickster\")\n```\n:::\n\n\nLoad the libraries we'll use throughout:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brickster)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'brickster' was built under R version 4.4.3\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(DBI)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n:::\n\n\n## Step 1: Setup Connection\n\nBrickster reads authentication credentials from environment variables. Store them in a `.env` file in your project root (or in `.Renviron` for global access) so they're never hard-coded in your scripts.\n\nCreate a `.env` file in your project root:\n\n```\nDATABRICKS_HOST=https://<your-workspace>.cloud.databricks.com\nDATABRICKS_TOKEN=<your-personal-access-token>\n```\n\n::: {.callout-tip title=\"Optional: SQL Warehouse ID\" appearance=\"simple\"}\nIf you know your SQL Warehouse ID, add `DATABRICKS_WAREHOUSE_ID=<id>` to your `.env` file. If not, brickster can auto-discover the first available warehouse in your workspace.\n:::\n\nOur project helper at `_databricks/connect.R` loads the `.env` file automatically and handles warehouse discovery:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"../../_databricks/connect.R\")\ncon <- get_databricks_connection()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nDATABRICKS_WAREHOUSE_ID not set, discovering warehouses...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing warehouse: Serverless Starter Warehouse (c732306bf3906f4d)\n```\n\n\n:::\n:::\n\n\nVerify the connection works:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbGetQuery(con, \"SELECT current_catalog(), current_schema()\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\ni Submitting query\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nv Submitting query [796ms]\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\ni Fetching 1 rows\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nv Downloaded 1 rows [521ms]\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\ni Processing results\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nv Processing results [12ms]\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 2\n  `current_catalog()` `current_schema()`\n  <chr>               <chr>             \n1 hive_metastore      default           \n```\n\n\n:::\n:::\n\n\n## Step 2: List Catalogs\n\nUnity Catalog organises data in a three-level namespace: **catalog** > **schema** > **table**. Let's start by listing the catalogs available in your workspace:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncatalogs <- db_uc_catalogs_list()\n```\n:::\n\n\nThis returns a list of all catalogs you have access to. You should see at least `main` (the default) and `samples` (Databricks sample data).\n\n::: {.callout-note title=\"Output: db_uc_catalogs_list()\" collapse=\"true\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[[1]]$name\n[1] \"samples\"\n\n[[1]]$owner\n[1] \"System user\"\n\n[[1]]$comment\n[1] \"These sample datasets are made available by third party data providers as well as open data sources. You can learn more about each data set by clicking on each one.\\n\\nTo discover more instantly available, free data sets across a wide range of industry use cases, visit [Databricks Marketplace](/marketplace).\\n\\nPlease note that the third party data sets represent a reduced portion of the available data attributes, volume, and data types available from providers, and are intended for educational rather than production purposes.\"\n\n[[1]]$catalog_type\n[1] \"SYSTEM_CATALOG\"\n\n[[1]]$metastore_id\n[1] \"473e68a6-17da-4dab-b4ab-c9959df3a2c5\"\n\n[[1]]$created_at\n[1] 1.768904e+12\n\n[[1]]$created_by\n[1] \"System user\"\n\n[[1]]$updated_at\n[1] 1.77146e+12\n\n[[1]]$updated_by\n[1] \"System user\"\n\n[[1]]$isolation_mode\n[1] \"OPEN\"\n\n[[1]]$accessible_in_current_workspace\n[1] TRUE\n\n[[1]]$browse_only\n[1] FALSE\n\n[[1]]$provisioning_info\n[[1]]$provisioning_info$state\n[1] \"ACTIVE\"\n\n\n[[1]]$id\n[1] \"7784a1df-de20-49da-8043-a75229b890fc\"\n\n[[1]]$full_name\n[1] \"samples\"\n\n[[1]]$securable_type\n[1] \"CATALOG\"\n\n[[1]]$securable_kind\n[1] \"CATALOG_SYSTEM\"\n\n[[1]]$resource_name\n[1] \"/metastores/473e68a6-17da-4dab-b4ab-c9959df3a2c5/catalogs/7784a1df-de20-49da-8043-a75229b890fc\"\n\n[[1]]$delta_sharing_valid_through_timestamp\n[1] 1.77146e+12\n\n[[1]]$metastore_version\n[1] -1\n\n[[1]]$cache_version_info\n[[1]]$cache_version_info$metastore_version\n[1] -1\n\n\n\n[[2]]\n[[2]]$name\n[1] \"system\"\n\n[[2]]$owner\n[1] \"System user\"\n\n[[2]]$comment\n[1] \"System catalog (auto-created)\"\n\n[[2]]$catalog_type\n[1] \"SYSTEM_CATALOG\"\n\n[[2]]$metastore_id\n[1] \"473e68a6-17da-4dab-b4ab-c9959df3a2c5\"\n\n[[2]]$created_at\n[1] 1.768904e+12\n\n[[2]]$created_by\n[1] \"System user\"\n\n[[2]]$updated_at\n[1] 1.770988e+12\n\n[[2]]$updated_by\n[1] \"System user\"\n\n[[2]]$isolation_mode\n[1] \"OPEN\"\n\n[[2]]$accessible_in_current_workspace\n[1] TRUE\n\n[[2]]$browse_only\n[1] FALSE\n\n[[2]]$provisioning_info\n[[2]]$provisioning_info$state\n[1] \"ACTIVE\"\n\n\n[[2]]$id\n[1] \"34f4e46c-9d9e-4a54-9217-fa01d21f5d13\"\n\n[[2]]$full_name\n[1] \"system\"\n\n[[2]]$securable_type\n[1] \"CATALOG\"\n\n[[2]]$securable_kind\n[1] \"CATALOG_SYSTEM\"\n\n[[2]]$resource_name\n[1] \"/metastores/473e68a6-17da-4dab-b4ab-c9959df3a2c5/catalogs/34f4e46c-9d9e-4a54-9217-fa01d21f5d13\"\n\n[[2]]$delta_sharing_valid_through_timestamp\n[1] 1.770988e+12\n\n[[2]]$metastore_version\n[1] -1\n\n[[2]]$cache_version_info\n[[2]]$cache_version_info$metastore_version\n[1] -1\n```\n\n\n:::\n:::\n\n:::\n\n## Step 3: Get a Catalog\n\nDrill into a specific catalog to see its metadata:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples_catalog <- db_uc_catalogs_get(catalog = \"samples\")\n```\n:::\n\n\nThis returns details like the catalog owner, creation time, and any comments attached to it.\n\n::: {.callout-note title=\"Output: db_uc_catalogs_get()\" collapse=\"true\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n$name\n[1] \"samples\"\n\n$owner\n[1] \"System user\"\n\n$comment\n[1] \"These sample datasets are made available by third party data providers as well as open data sources. You can learn more about each data set by clicking on each one.\\n\\nTo discover more instantly available, free data sets across a wide range of industry use cases, visit [Databricks Marketplace](/marketplace).\\n\\nPlease note that the third party data sets represent a reduced portion of the available data attributes, volume, and data types available from providers, and are intended for educational rather than production purposes.\"\n\n$catalog_type\n[1] \"SYSTEM_CATALOG\"\n\n$metastore_id\n[1] \"473e68a6-17da-4dab-b4ab-c9959df3a2c5\"\n\n$created_at\n[1] 1.768904e+12\n\n$created_by\n[1] \"System user\"\n\n$updated_at\n[1] 1.77146e+12\n\n$updated_by\n[1] \"System user\"\n\n$isolation_mode\n[1] \"OPEN\"\n\n$browse_only\n[1] FALSE\n\n$provisioning_info\n$provisioning_info$state\n[1] \"ACTIVE\"\n\n\n$id\n[1] \"7784a1df-de20-49da-8043-a75229b890fc\"\n\n$full_name\n[1] \"samples\"\n\n$securable_type\n[1] \"CATALOG\"\n\n$securable_kind\n[1] \"CATALOG_SYSTEM\"\n\n$resource_name\n[1] \"/metastores/473e68a6-17da-4dab-b4ab-c9959df3a2c5/catalogs/7784a1df-de20-49da-8043-a75229b890fc\"\n\n$delta_sharing_valid_through_timestamp\n[1] 1.77146e+12\n\n$metastore_version\n[1] 199\n\n$cache_version_info\n$cache_version_info$metastore_version\n[1] 199\n```\n\n\n:::\n:::\n\n:::\n\n## Step 4: List Tables in a Schema\n\nNow let's navigate into the Bakehouse schema and see what tables are available. First, list the schemas in the `samples` catalog:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschemas <- db_uc_schemas_list(catalog = \"samples\")\n```\n:::\n\n\n::: {.callout-note title=\"Output: db_uc_schemas_list()\" collapse=\"true\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[[1]]$name\n[1] \"accuweather\"\n\n[[1]]$catalog_name\n[1] \"samples\"\n\n[[1]]$owner\n[1] \"System user\"\n\n[[1]]$metastore_id\n[1] \"473e68a6-17da-4dab-b4ab-c9959df3a2c5\"\n\n[[1]]$full_name\n[1] \"samples.accuweather\"\n\n[[1]]$created_at\n[1] 1.768904e+12\n\n[[1]]$created_by\n[1] \"System user\"\n\n[[1]]$updated_at\n[1] 1.768904e+12\n\n[[1]]$updated_by\n[1] \"System user\"\n\n[[1]]$catalog_type\n[1] \"SYSTEM_CATALOG\"\n\n[[1]]$schema_id\n[1] \"1cd49a6c-8b95-4430-8001-ff5ccc82f2ab\"\n\n[[1]]$delta_sharing_valid_through_timestamp\n[1] 1.768904e+12\n\n[[1]]$securable_type\n[1] \"SCHEMA\"\n\n[[1]]$securable_kind\n[1] \"SCHEMA_SYSTEM_DELTASHARING\"\n\n[[1]]$browse_only\n[1] FALSE\n\n[[1]]$metastore_version\n[1] -1\n\n[[1]]$provider_id\n[1] \"5882aec0-be20-4493-9bad-d04f93c1c2ce\"\n\n[[1]]$share_name\n[1] \"system_schemas_accuweather_share\"\n\n[[1]]$cache_version_info\n[[1]]$cache_version_info$metastore_version\n[1] -1\n\n\n\n[[2]]\n[[2]]$name\n[1] \"bakehouse\"\n\n[[2]]$catalog_name\n[1] \"samples\"\n\n[[2]]$owner\n[1] \"System user\"\n\n[[2]]$metastore_id\n[1] \"473e68a6-17da-4dab-b4ab-c9959df3a2c5\"\n\n[[2]]$full_name\n[1] \"samples.bakehouse\"\n\n[[2]]$created_at\n[1] 1.768904e+12\n\n[[2]]$created_by\n[1] \"System user\"\n\n[[2]]$updated_at\n[1] 1.77146e+12\n\n[[2]]$updated_by\n[1] \"System user\"\n\n[[2]]$catalog_type\n[1] \"SYSTEM_CATALOG\"\n\n[[2]]$provisioning_info\n[[2]]$provisioning_info$state\n[1] \"ACTIVE\"\n\n\n[[2]]$schema_id\n[1] \"9973e8cf-dec7-4b0b-b2ab-615724b9d07b\"\n\n[[2]]$delta_sharing_valid_through_timestamp\n[1] 1.77146e+12\n\n[[2]]$securable_type\n[1] \"SCHEMA\"\n\n[[2]]$securable_kind\n[1] \"SCHEMA_SYSTEM_DELTASHARING\"\n\n[[2]]$browse_only\n[1] FALSE\n\n[[2]]$metastore_version\n[1] -1\n\n[[2]]$provider_id\n[1] \"5882aec0-be20-4493-9bad-d04f93c1c2ce\"\n\n[[2]]$share_name\n[1] \"system_schemas_bakehouse_share\"\n\n[[2]]$cache_version_info\n[[2]]$cache_version_info$metastore_version\n[1] -1\n\n\n\n[[3]]\n[[3]]$name\n[1] \"healthverity\"\n\n[[3]]$catalog_name\n[1] \"samples\"\n\n[[3]]$owner\n[1] \"System user\"\n\n[[3]]$metastore_id\n[1] \"473e68a6-17da-4dab-b4ab-c9959df3a2c5\"\n\n[[3]]$full_name\n[1] \"samples.healthverity\"\n\n[[3]]$created_at\n[1] 1.768905e+12\n\n[[3]]$created_by\n[1] \"System user\"\n\n[[3]]$updated_at\n[1] 1.76912e+12\n\n[[3]]$updated_by\n[1] \"System user\"\n\n[[3]]$catalog_type\n[1] \"SYSTEM_CATALOG\"\n\n[[3]]$provisioning_info\n[[3]]$provisioning_info$state\n[1] \"ACTIVE\"\n\n\n[[3]]$schema_id\n[1] \"0e15daec-007e-46ef-8df6-cabe58e10b55\"\n\n[[3]]$delta_sharing_valid_through_timestamp\n[1] 1.76912e+12\n\n[[3]]$securable_type\n[1] \"SCHEMA\"\n\n[[3]]$securable_kind\n[1] \"SCHEMA_SYSTEM_DELTASHARING\"\n\n[[3]]$browse_only\n[1] FALSE\n\n[[3]]$metastore_version\n[1] -1\n\n[[3]]$provider_id\n[1] \"5882aec0-be20-4493-9bad-d04f93c1c2ce\"\n\n[[3]]$share_name\n[1] \"system_schemas_healthverity_share\"\n\n[[3]]$cache_version_info\n[[3]]$cache_version_info$metastore_version\n[1] -1\n\n\n\n[[4]]\n[[4]]$name\n[1] \"information_schema\"\n\n[[4]]$catalog_name\n[1] \"samples\"\n\n[[4]]$owner\n[1] \"System user\"\n\n[[4]]$comment\n[1] \"Information schema (auto-created)\"\n\n[[4]]$metastore_id\n[1] \"473e68a6-17da-4dab-b4ab-c9959df3a2c5\"\n\n[[4]]$full_name\n[1] \"samples.information_schema\"\n\n[[4]]$created_at\n[1] 1.768904e+12\n\n[[4]]$created_by\n[1] \"System user\"\n\n[[4]]$updated_at\n[1] 1.768904e+12\n\n[[4]]$updated_by\n[1] \"System user\"\n\n[[4]]$catalog_type\n[1] \"SYSTEM_CATALOG\"\n\n[[4]]$schema_id\n[1] \"346bca9d-fb45-421c-bb5d-c584b3f692d9\"\n\n[[4]]$securable_type\n[1] \"SCHEMA\"\n\n[[4]]$securable_kind\n[1] \"SCHEMA_SYSTEM\"\n\n[[4]]$browse_only\n[1] FALSE\n\n[[4]]$metastore_version\n[1] -1\n\n[[4]]$cache_version_info\n[[4]]$cache_version_info$metastore_version\n[1] -1\n\n\n\n[[5]]\n[[5]]$name\n[1] \"nyctaxi\"\n\n[[5]]$catalog_name\n[1] \"samples\"\n\n[[5]]$owner\n[1] \"System user\"\n\n[[5]]$metastore_id\n[1] \"473e68a6-17da-4dab-b4ab-c9959df3a2c5\"\n\n[[5]]$full_name\n[1] \"samples.nyctaxi\"\n\n[[5]]$created_at\n[1] 1.768904e+12\n\n[[5]]$created_by\n[1] \"System user\"\n\n[[5]]$updated_at\n[1] 1.77146e+12\n\n[[5]]$updated_by\n[1] \"System user\"\n\n[[5]]$catalog_type\n[1] \"SYSTEM_CATALOG\"\n\n[[5]]$provisioning_info\n[[5]]$provisioning_info$state\n[1] \"ACTIVE\"\n\n\n[[5]]$schema_id\n[1] \"a3c86988-adf7-405f-9338-543fd8870a69\"\n\n[[5]]$delta_sharing_valid_through_timestamp\n[1] 1.77146e+12\n\n[[5]]$securable_type\n[1] \"SCHEMA\"\n\n[[5]]$securable_kind\n[1] \"SCHEMA_SYSTEM_DELTASHARING\"\n\n[[5]]$browse_only\n[1] FALSE\n\n[[5]]$metastore_version\n[1] -1\n\n[[5]]$provider_id\n[1] \"5882aec0-be20-4493-9bad-d04f93c1c2ce\"\n\n[[5]]$share_name\n[1] \"system_schemas_nyctaxi_share\"\n\n[[5]]$cache_version_info\n[[5]]$cache_version_info$metastore_version\n[1] -1\n\n\n\n[[6]]\n[[6]]$name\n[1] \"tpcds_sf1\"\n\n[[6]]$catalog_name\n[1] \"samples\"\n\n[[6]]$owner\n[1] \"System user\"\n\n[[6]]$metastore_id\n[1] \"473e68a6-17da-4dab-b4ab-c9959df3a2c5\"\n\n[[6]]$full_name\n[1] \"samples.tpcds_sf1\"\n\n[[6]]$created_at\n[1] 1.768905e+12\n\n[[6]]$created_by\n[1] \"System user\"\n\n[[6]]$updated_at\n[1] 1.768905e+12\n\n[[6]]$updated_by\n[1] \"System user\"\n\n[[6]]$catalog_type\n[1] \"SYSTEM_CATALOG\"\n\n[[6]]$schema_id\n[1] \"783d4867-db8f-40db-8abe-ef0513c03733\"\n\n[[6]]$delta_sharing_valid_through_timestamp\n[1] 1.768905e+12\n\n[[6]]$securable_type\n[1] \"SCHEMA\"\n\n[[6]]$securable_kind\n[1] \"SCHEMA_SYSTEM_DELTASHARING\"\n\n[[6]]$browse_only\n[1] FALSE\n\n[[6]]$metastore_version\n[1] -1\n\n[[6]]$provider_id\n[1] \"5882aec0-be20-4493-9bad-d04f93c1c2ce\"\n\n[[6]]$share_name\n[1] \"system_schemas_tpcds_sf1_share\"\n\n[[6]]$cache_version_info\n[[6]]$cache_version_info$metastore_version\n[1] -1\n\n\n\n[[7]]\n[[7]]$name\n[1] \"tpcds_sf1000\"\n\n[[7]]$catalog_name\n[1] \"samples\"\n\n[[7]]$owner\n[1] \"System user\"\n\n[[7]]$metastore_id\n[1] \"473e68a6-17da-4dab-b4ab-c9959df3a2c5\"\n\n[[7]]$full_name\n[1] \"samples.tpcds_sf1000\"\n\n[[7]]$created_at\n[1] 1.768905e+12\n\n[[7]]$created_by\n[1] \"System user\"\n\n[[7]]$updated_at\n[1] 1.768905e+12\n\n[[7]]$updated_by\n[1] \"System user\"\n\n[[7]]$catalog_type\n[1] \"SYSTEM_CATALOG\"\n\n[[7]]$schema_id\n[1] \"a1a0b9a1-2103-46e1-97d2-60e36b1e07a4\"\n\n[[7]]$delta_sharing_valid_through_timestamp\n[1] 1.768905e+12\n\n[[7]]$securable_type\n[1] \"SCHEMA\"\n\n[[7]]$securable_kind\n[1] \"SCHEMA_SYSTEM_DELTASHARING\"\n\n[[7]]$browse_only\n[1] FALSE\n\n[[7]]$metastore_version\n[1] -1\n\n[[7]]$provider_id\n[1] \"5882aec0-be20-4493-9bad-d04f93c1c2ce\"\n\n[[7]]$share_name\n[1] \"system_schemas_tpcds_sf1000_share\"\n\n[[7]]$cache_version_info\n[[7]]$cache_version_info$metastore_version\n[1] -1\n\n\n\n[[8]]\n[[8]]$name\n[1] \"tpch\"\n\n[[8]]$catalog_name\n[1] \"samples\"\n\n[[8]]$owner\n[1] \"System user\"\n\n[[8]]$metastore_id\n[1] \"473e68a6-17da-4dab-b4ab-c9959df3a2c5\"\n\n[[8]]$full_name\n[1] \"samples.tpch\"\n\n[[8]]$created_at\n[1] 1.768904e+12\n\n[[8]]$created_by\n[1] \"System user\"\n\n[[8]]$updated_at\n[1] 1.77146e+12\n\n[[8]]$updated_by\n[1] \"System user\"\n\n[[8]]$catalog_type\n[1] \"SYSTEM_CATALOG\"\n\n[[8]]$provisioning_info\n[[8]]$provisioning_info$state\n[1] \"ACTIVE\"\n\n\n[[8]]$schema_id\n[1] \"a1ebcf87-a2b1-4c3e-8843-f71b2a9eebd3\"\n\n[[8]]$delta_sharing_valid_through_timestamp\n[1] 1.77146e+12\n\n[[8]]$securable_type\n[1] \"SCHEMA\"\n\n[[8]]$securable_kind\n[1] \"SCHEMA_SYSTEM_DELTASHARING\"\n\n[[8]]$browse_only\n[1] FALSE\n\n[[8]]$metastore_version\n[1] -1\n\n[[8]]$provider_id\n[1] \"5882aec0-be20-4493-9bad-d04f93c1c2ce\"\n\n[[8]]$share_name\n[1] \"system_schemas_tpch_share\"\n\n[[8]]$cache_version_info\n[[8]]$cache_version_info$metastore_version\n[1] -1\n\n\n\n[[9]]\n[[9]]$name\n[1] \"wanderbricks\"\n\n[[9]]$catalog_name\n[1] \"samples\"\n\n[[9]]$owner\n[1] \"System user\"\n\n[[9]]$metastore_id\n[1] \"473e68a6-17da-4dab-b4ab-c9959df3a2c5\"\n\n[[9]]$full_name\n[1] \"samples.wanderbricks\"\n\n[[9]]$created_at\n[1] 1.768905e+12\n\n[[9]]$created_by\n[1] \"System user\"\n\n[[9]]$updated_at\n[1] 1.768905e+12\n\n[[9]]$updated_by\n[1] \"System user\"\n\n[[9]]$catalog_type\n[1] \"SYSTEM_CATALOG\"\n\n[[9]]$schema_id\n[1] \"3d68a6ae-c0fc-4020-b439-d7fe375e42f7\"\n\n[[9]]$delta_sharing_valid_through_timestamp\n[1] 1.768905e+12\n\n[[9]]$securable_type\n[1] \"SCHEMA\"\n\n[[9]]$securable_kind\n[1] \"SCHEMA_SYSTEM_DELTASHARING\"\n\n[[9]]$browse_only\n[1] FALSE\n\n[[9]]$metastore_version\n[1] -1\n\n[[9]]$provider_id\n[1] \"5882aec0-be20-4493-9bad-d04f93c1c2ce\"\n\n[[9]]$share_name\n[1] \"system_schemas_wanderbricks_share\"\n\n[[9]]$cache_version_info\n[[9]]$cache_version_info$metastore_version\n[1] -1\n```\n\n\n:::\n:::\n\n:::\n\nThen list the tables in the `bakehouse` schema:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntables <- db_uc_tables_list(catalog = \"samples\", schema = \"bakehouse\")\n```\n:::\n\n\nYou should see tables like `sales_transactions`, `sales_customers`, `sales_franchises`, and `sales_suppliers`.\n\n::: {.callout-note title=\"Output: db_uc_tables_list()\" collapse=\"true\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[[1]]$name\n[1] \"media_customer_reviews\"\n\n[[1]]$catalog_name\n[1] \"samples\"\n\n[[1]]$schema_name\n[1] \"bakehouse\"\n\n[[1]]$table_type\n[1] \"MANAGED\"\n\n[[1]]$data_source_format\n[1] \"DELTA\"\n\n[[1]]$storage_location\n[1] \"abfss://metastore@ucstprdeastus.dfs.core.windows.net/92d417bd-c99d-4fc6-8cb2-aada3029ea89/tables/71bc6830-bf01-4a23-92d9-68c067021781\"\n\n[[1]]$comment\n[1] \"The **Bakehouse Dataset** simulates a bakery franchise business and contains several key datasets for various analytical and AI-driven use cases. Please note that this sample dataset has been synthetically curated and is suitable for any Databricks workload.\\n\\nSample use cases for this dataset include:\\n\\n* **Building Data Pipelines with Delta Live Tables**: Create automated, real-time data pipelines for efficient data ingestion, transformation, and management.\\n* **Performing Analytics with Databricks SQL**: Conduct powerful SQL-based analytics to uncover actionable insights from structured data, including sales trends and customer behavior.\\n* **Exploring AI and Machine Learning Capabilities**: Use the dataset to develop and train machine learning models, applying AI to forecast trends, optimize operations, and predict customer preferences.\"\n\n[[1]]$securable_kind\n[1] \"TABLE_DELTASHARING\"\n\n[[1]]$generation\n[1] 0\n\n[[1]]$metastore_id\n[1] \"473e68a6-17da-4dab-b4ab-c9959df3a2c5\"\n\n[[1]]$full_name\n[1] \"samples.bakehouse.media_customer_reviews\"\n\n[[1]]$data_access_configuration_id\n[1] \"00000000-0000-0000-0000-000000000000\"\n\n[[1]]$created_at\n[1] 1.768904e+12\n\n[[1]]$updated_at\n[1] 1.768904e+12\n\n[[1]]$table_id\n[1] \"3869ffc3-841a-4aab-b199-1ff8a30ff624\"\n\n[[1]]$delta_runtime_properties_kvpairs\nnamed list()\n\n[[1]]$securable_type\n[1] \"TABLE\"\n\n[[1]]$browse_only\n[1] FALSE\n\n[[1]]$delta_sharing_type\nnamed list()\n\n[[1]]$delta_sharing_valid_through_timestamp\n[1] 1.768904e+12\n\n[[1]]$metastore_version\n[1] -1\n\n[[1]]$share_name\n[1] \"system_schemas_bakehouse_share\"\n\n[[1]]$provider_id\n[1] \"5882aec0-be20-4493-9bad-d04f93c1c2ce\"\n\n[[1]]$schema_id\n[1] \"9973e8cf-dec7-4b0b-b2ab-615724b9d07b\"\n\n[[1]]$catalog_id\n[1] \"7784a1df-de20-49da-8043-a75229b890fc\"\n\n[[1]]$row_filters\nnamed list()\n\n[[1]]$cache_version_info\n[[1]]$cache_version_info$metastore_version\n[1] -1\n\n[[1]]$cache_version_info$security_policy_checksum\n[1] -1\n\n\n[[1]]$etag\n[1] \"CAESCAAAAZva5HQp\"\n\n\n[[2]]\n[[2]]$name\n[1] \"media_gold_reviews_chunked\"\n\n[[2]]$catalog_name\n[1] \"samples\"\n\n[[2]]$schema_name\n[1] \"bakehouse\"\n\n[[2]]$table_type\n[1] \"MANAGED\"\n\n[[2]]$data_source_format\n[1] \"DELTA\"\n\n[[2]]$storage_location\n[1] \"abfss://metastore@ucstprdeastus.dfs.core.windows.net/92d417bd-c99d-4fc6-8cb2-aada3029ea89/tables/0bcc8668-1f42-4bef-928f-1c3e7aa9bf14\"\n\n[[2]]$comment\n[1] \"The **Bakehouse Dataset** simulates a bakery franchise business and contains several key datasets for various analytical and AI-driven use cases. Please note that this sample dataset has been synthetically curated and is suitable for any Databricks workload.\\n\\nSample use cases for this dataset include:\\n\\n* **Building Data Pipelines with Delta Live Tables**: Create automated, real-time data pipelines for efficient data ingestion, transformation, and management.\\n* **Performing Analytics with Databricks SQL**: Conduct powerful SQL-based analytics to uncover actionable insights from structured data, including sales trends and customer behavior.\\n* **Exploring AI and Machine Learning Capabilities**: Use the dataset to develop and train machine learning models, applying AI to forecast trends, optimize operations, and predict customer preferences.\"\n\n[[2]]$securable_kind\n[1] \"TABLE_DELTASHARING\"\n\n[[2]]$generation\n[1] 0\n\n[[2]]$metastore_id\n[1] \"473e68a6-17da-4dab-b4ab-c9959df3a2c5\"\n\n[[2]]$full_name\n[1] \"samples.bakehouse.media_gold_reviews_chunked\"\n\n[[2]]$data_access_configuration_id\n[1] \"00000000-0000-0000-0000-000000000000\"\n\n[[2]]$created_at\n[1] 1.768904e+12\n\n[[2]]$updated_at\n[1] 1.768904e+12\n\n[[2]]$table_id\n[1] \"04fea062-3838-4d8c-b094-dfddb08b72fe\"\n\n[[2]]$delta_runtime_properties_kvpairs\nnamed list()\n\n[[2]]$securable_type\n[1] \"TABLE\"\n\n[[2]]$browse_only\n[1] FALSE\n\n[[2]]$delta_sharing_type\nnamed list()\n\n[[2]]$delta_sharing_valid_through_timestamp\n[1] 1.768904e+12\n\n[[2]]$metastore_version\n[1] -1\n\n[[2]]$share_name\n[1] \"system_schemas_bakehouse_share\"\n\n[[2]]$provider_id\n[1] \"5882aec0-be20-4493-9bad-d04f93c1c2ce\"\n\n[[2]]$schema_id\n[1] \"9973e8cf-dec7-4b0b-b2ab-615724b9d07b\"\n\n[[2]]$catalog_id\n[1] \"7784a1df-de20-49da-8043-a75229b890fc\"\n\n[[2]]$row_filters\nnamed list()\n\n[[2]]$cache_version_info\n[[2]]$cache_version_info$metastore_version\n[1] -1\n\n[[2]]$cache_version_info$security_policy_checksum\n[1] -1\n\n\n[[2]]$etag\n[1] \"CAESCAAAAZva5HQ5\"\n\n\n[[3]]\n[[3]]$name\n[1] \"sales_customers\"\n\n[[3]]$catalog_name\n[1] \"samples\"\n\n[[3]]$schema_name\n[1] \"bakehouse\"\n\n[[3]]$table_type\n[1] \"MANAGED\"\n\n[[3]]$data_source_format\n[1] \"DELTA\"\n\n[[3]]$storage_location\n[1] \"abfss://metastore@ucstprdeastus.dfs.core.windows.net/92d417bd-c99d-4fc6-8cb2-aada3029ea89/tables/4258b0c6-b4d8-4b2d-8a5d-26b979c83100\"\n\n[[3]]$comment\n[1] \"The **Bakehouse Dataset** simulates a bakery franchise business and contains several key datasets for various analytical and AI-driven use cases. Please note that this sample dataset has been synthetically curated and is suitable for any Databricks workload.\\n\\nSample use cases for this dataset include:\\n\\n* **Building Data Pipelines with Delta Live Tables**: Create automated, real-time data pipelines for efficient data ingestion, transformation, and management.\\n* **Performing Analytics with Databricks SQL**: Conduct powerful SQL-based analytics to uncover actionable insights from structured data, including sales trends and customer behavior.\\n* **Exploring AI and Machine Learning Capabilities**: Use the dataset to develop and train machine learning models, applying AI to forecast trends, optimize operations, and predict customer preferences.\"\n\n[[3]]$securable_kind\n[1] \"TABLE_DELTASHARING\"\n\n[[3]]$generation\n[1] 18\n\n[[3]]$metastore_id\n[1] \"473e68a6-17da-4dab-b4ab-c9959df3a2c5\"\n\n[[3]]$full_name\n[1] \"samples.bakehouse.sales_customers\"\n\n[[3]]$data_access_configuration_id\n[1] \"00000000-0000-0000-0000-000000000000\"\n\n[[3]]$created_at\n[1] 1.768904e+12\n\n[[3]]$updated_at\n[1] 1.771447e+12\n\n[[3]]$table_id\n[1] \"faf6d936-9a77-4e54-99f7-82770f1643ba\"\n\n[[3]]$delta_runtime_properties_kvpairs\nnamed list()\n\n[[3]]$securable_type\n[1] \"TABLE\"\n\n[[3]]$browse_only\n[1] FALSE\n\n[[3]]$provisioning_info\n[[3]]$provisioning_info$state\n[1] \"ACTIVE\"\n\n\n[[3]]$delta_sharing_type\nnamed list()\n\n[[3]]$delta_sharing_valid_through_timestamp\n[1] 1.771447e+12\n\n[[3]]$metastore_version\n[1] -1\n\n[[3]]$share_name\n[1] \"system_schemas_bakehouse_share\"\n\n[[3]]$provider_id\n[1] \"5882aec0-be20-4493-9bad-d04f93c1c2ce\"\n\n[[3]]$schema_id\n[1] \"9973e8cf-dec7-4b0b-b2ab-615724b9d07b\"\n\n[[3]]$catalog_id\n[1] \"7784a1df-de20-49da-8043-a75229b890fc\"\n\n[[3]]$row_filters\nnamed list()\n\n[[3]]$cache_version_info\n[[3]]$cache_version_info$metastore_version\n[1] -1\n\n[[3]]$cache_version_info$security_policy_checksum\n[1] -1\n\n\n[[3]]$etag\n[1] \"CAESCAAAAZxyeAwT\"\n\n\n[[4]]\n[[4]]$name\n[1] \"sales_franchises\"\n\n[[4]]$catalog_name\n[1] \"samples\"\n\n[[4]]$schema_name\n[1] \"bakehouse\"\n\n[[4]]$table_type\n[1] \"MANAGED\"\n\n[[4]]$data_source_format\n[1] \"DELTA\"\n\n[[4]]$storage_location\n[1] \"abfss://metastore@ucstprdeastus.dfs.core.windows.net/92d417bd-c99d-4fc6-8cb2-aada3029ea89/tables/b086ba4c-f190-4c91-b023-9c013079db34\"\n\n[[4]]$comment\n[1] \"The **Bakehouse Dataset** simulates a bakery franchise business and contains several key datasets for various analytical and AI-driven use cases. Please note that this sample dataset has been synthetically curated and is suitable for any Databricks workload.\\n\\nSample use cases for this dataset include:\\n\\n* **Building Data Pipelines with Delta Live Tables**: Create automated, real-time data pipelines for efficient data ingestion, transformation, and management.\\n* **Performing Analytics with Databricks SQL**: Conduct powerful SQL-based analytics to uncover actionable insights from structured data, including sales trends and customer behavior.\\n* **Exploring AI and Machine Learning Capabilities**: Use the dataset to develop and train machine learning models, applying AI to forecast trends, optimize operations, and predict customer preferences.\"\n\n[[4]]$securable_kind\n[1] \"TABLE_DELTASHARING\"\n\n[[4]]$generation\n[1] 0\n\n[[4]]$metastore_id\n[1] \"473e68a6-17da-4dab-b4ab-c9959df3a2c5\"\n\n[[4]]$full_name\n[1] \"samples.bakehouse.sales_franchises\"\n\n[[4]]$data_access_configuration_id\n[1] \"00000000-0000-0000-0000-000000000000\"\n\n[[4]]$created_at\n[1] 1.768904e+12\n\n[[4]]$updated_at\n[1] 1.768904e+12\n\n[[4]]$table_id\n[1] \"9d2a4133-9b39-447f-9859-e39fdd910c5c\"\n\n[[4]]$delta_runtime_properties_kvpairs\nnamed list()\n\n[[4]]$securable_type\n[1] \"TABLE\"\n\n[[4]]$browse_only\n[1] FALSE\n\n[[4]]$delta_sharing_type\nnamed list()\n\n[[4]]$delta_sharing_valid_through_timestamp\n[1] 1.768904e+12\n\n[[4]]$metastore_version\n[1] -1\n\n[[4]]$share_name\n[1] \"system_schemas_bakehouse_share\"\n\n[[4]]$provider_id\n[1] \"5882aec0-be20-4493-9bad-d04f93c1c2ce\"\n\n[[4]]$schema_id\n[1] \"9973e8cf-dec7-4b0b-b2ab-615724b9d07b\"\n\n[[4]]$catalog_id\n[1] \"7784a1df-de20-49da-8043-a75229b890fc\"\n\n[[4]]$row_filters\nnamed list()\n\n[[4]]$cache_version_info\n[[4]]$cache_version_info$metastore_version\n[1] -1\n\n[[4]]$cache_version_info$security_policy_checksum\n[1] -1\n\n\n[[4]]$etag\n[1] \"CAESCAAAAZva5HRY\"\n\n\n[[5]]\n[[5]]$name\n[1] \"sales_suppliers\"\n\n[[5]]$catalog_name\n[1] \"samples\"\n\n[[5]]$schema_name\n[1] \"bakehouse\"\n\n[[5]]$table_type\n[1] \"MANAGED\"\n\n[[5]]$data_source_format\n[1] \"DELTA\"\n\n[[5]]$storage_location\n[1] \"abfss://metastore@ucstprdeastus.dfs.core.windows.net/92d417bd-c99d-4fc6-8cb2-aada3029ea89/tables/4006b092-829a-4c8a-b373-17c7081df717\"\n\n[[5]]$comment\n[1] \"The **Bakehouse Dataset** simulates a bakery franchise business and contains several key datasets for various analytical and AI-driven use cases. Please note that this sample dataset has been synthetically curated and is suitable for any Databricks workload.\\n\\nSample use cases for this dataset include:\\n\\n* **Building Data Pipelines with Delta Live Tables**: Create automated, real-time data pipelines for efficient data ingestion, transformation, and management.\\n* **Performing Analytics with Databricks SQL**: Conduct powerful SQL-based analytics to uncover actionable insights from structured data, including sales trends and customer behavior.\\n* **Exploring AI and Machine Learning Capabilities**: Use the dataset to develop and train machine learning models, applying AI to forecast trends, optimize operations, and predict customer preferences.\"\n\n[[5]]$securable_kind\n[1] \"TABLE_DELTASHARING\"\n\n[[5]]$generation\n[1] 17\n\n[[5]]$metastore_id\n[1] \"473e68a6-17da-4dab-b4ab-c9959df3a2c5\"\n\n[[5]]$full_name\n[1] \"samples.bakehouse.sales_suppliers\"\n\n[[5]]$data_access_configuration_id\n[1] \"00000000-0000-0000-0000-000000000000\"\n\n[[5]]$created_at\n[1] 1.768904e+12\n\n[[5]]$updated_at\n[1] 1.770137e+12\n\n[[5]]$table_id\n[1] \"a9c33db9-d168-420a-bd5d-1a704e514f89\"\n\n[[5]]$delta_runtime_properties_kvpairs\nnamed list()\n\n[[5]]$securable_type\n[1] \"TABLE\"\n\n[[5]]$browse_only\n[1] FALSE\n\n[[5]]$provisioning_info\n[[5]]$provisioning_info$state\n[1] \"ACTIVE\"\n\n\n[[5]]$delta_sharing_type\nnamed list()\n\n[[5]]$delta_sharing_valid_through_timestamp\n[1] 1.770137e+12\n\n[[5]]$metastore_version\n[1] -1\n\n[[5]]$share_name\n[1] \"system_schemas_bakehouse_share\"\n\n[[5]]$provider_id\n[1] \"5882aec0-be20-4493-9bad-d04f93c1c2ce\"\n\n[[5]]$schema_id\n[1] \"9973e8cf-dec7-4b0b-b2ab-615724b9d07b\"\n\n[[5]]$catalog_id\n[1] \"7784a1df-de20-49da-8043-a75229b890fc\"\n\n[[5]]$row_filters\nnamed list()\n\n[[5]]$cache_version_info\n[[5]]$cache_version_info$metastore_version\n[1] -1\n\n[[5]]$cache_version_info$security_policy_checksum\n[1] -1\n\n\n[[5]]$etag\n[1] \"CAESCAAAAZwkYhfj\"\n\n\n[[6]]\n[[6]]$name\n[1] \"sales_transactions\"\n\n[[6]]$catalog_name\n[1] \"samples\"\n\n[[6]]$schema_name\n[1] \"bakehouse\"\n\n[[6]]$table_type\n[1] \"MANAGED\"\n\n[[6]]$data_source_format\n[1] \"DELTA\"\n\n[[6]]$storage_location\n[1] \"abfss://metastore@ucstprdeastus.dfs.core.windows.net/92d417bd-c99d-4fc6-8cb2-aada3029ea89/tables/55ef1811-6327-47b1-a369-cc045a32289c\"\n\n[[6]]$comment\n[1] \"The **Bakehouse Dataset** simulates a bakery franchise business and contains several key datasets for various analytical and AI-driven use cases. Please note that this sample dataset has been synthetically curated and is suitable for any Databricks workload.\\n\\nSample use cases for this dataset include:\\n\\n* **Building Data Pipelines with Delta Live Tables**: Create automated, real-time data pipelines for efficient data ingestion, transformation, and management.\\n* **Performing Analytics with Databricks SQL**: Conduct powerful SQL-based analytics to uncover actionable insights from structured data, including sales trends and customer behavior.\\n* **Exploring AI and Machine Learning Capabilities**: Use the dataset to develop and train machine learning models, applying AI to forecast trends, optimize operations, and predict customer preferences.\"\n\n[[6]]$securable_kind\n[1] \"TABLE_DELTASHARING\"\n\n[[6]]$generation\n[1] 36\n\n[[6]]$metastore_id\n[1] \"473e68a6-17da-4dab-b4ab-c9959df3a2c5\"\n\n[[6]]$full_name\n[1] \"samples.bakehouse.sales_transactions\"\n\n[[6]]$data_access_configuration_id\n[1] \"00000000-0000-0000-0000-000000000000\"\n\n[[6]]$created_at\n[1] 1.768904e+12\n\n[[6]]$updated_at\n[1] 1.77146e+12\n\n[[6]]$table_id\n[1] \"1d45c69a-ec10-46f8-abe0-2106bd12800a\"\n\n[[6]]$delta_runtime_properties_kvpairs\nnamed list()\n\n[[6]]$securable_type\n[1] \"TABLE\"\n\n[[6]]$browse_only\n[1] FALSE\n\n[[6]]$provisioning_info\n[[6]]$provisioning_info$state\n[1] \"ACTIVE\"\n\n\n[[6]]$delta_sharing_type\nnamed list()\n\n[[6]]$delta_sharing_valid_through_timestamp\n[1] 1.77146e+12\n\n[[6]]$metastore_version\n[1] -1\n\n[[6]]$share_name\n[1] \"system_schemas_bakehouse_share\"\n\n[[6]]$provider_id\n[1] \"5882aec0-be20-4493-9bad-d04f93c1c2ce\"\n\n[[6]]$schema_id\n[1] \"9973e8cf-dec7-4b0b-b2ab-615724b9d07b\"\n\n[[6]]$catalog_id\n[1] \"7784a1df-de20-49da-8043-a75229b890fc\"\n\n[[6]]$row_filters\nnamed list()\n\n[[6]]$cache_version_info\n[[6]]$cache_version_info$metastore_version\n[1] -1\n\n[[6]]$cache_version_info$security_policy_checksum\n[1] -1\n\n\n[[6]]$etag\n[1] \"CAESCAAAAZxzOjRe\"\n```\n\n\n:::\n:::\n\n:::\n\n::: {.callout-tip title=\"Quick Table Check\" appearance=\"simple\"}\nYou can also check if a specific table exists:\n\n```r\ndb_uc_tables_exists(catalog = \"samples\", schema = \"bakehouse\", table = \"sales_transactions\")\n```\n:::\n\n## Step 5: Install dplyr on the Cluster\n\nIf you're working with a Databricks cluster (rather than just a SQL Warehouse), you can install R packages directly onto it. Let's install the latest version of dplyr:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncluster_id <- Sys.getenv(\"DATABRICKS_CLUSTER_ID\")\n\ndb_libs_install(\n  cluster_id = cluster_id,\n  libraries = list(\n    list(cran = list(package = \"dplyr\"))\n  )\n)\n\n# Wait for the installation to complete\nwait_for_lib_installs(cluster_id = cluster_id)\n```\n:::\n\n\nCheck the installation status:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndb_libs_cluster_status(cluster_id = cluster_id)\n```\n:::\n\n\n::: {.callout-note title=\"SQL Warehouse vs Cluster\" appearance=\"simple\"}\nSQL Warehouses don't support library installation — they come pre-configured. The `db_libs_install()` function is for **all-purpose clusters** where you need custom R packages. For this tutorial, all remaining examples use the SQL Warehouse connection via DBI, which doesn't require cluster library installation.\n:::\n\n## Step 6: Run a SQL Query\n\nThe most direct way to query data is with `dbGetQuery()`. Let's pull the first 10 sales transactions from the Bakehouse dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsales <- dbGetQuery(\n  con,\n  \"SELECT * FROM samples.bakehouse.sales_transactions LIMIT 10\"\n)\n```\n:::\n\n\n::: {.callout-note title=\"Output: SELECT * ... LIMIT 10\" collapse=\"true\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 x 10\n   transactionID customerID franchiseID dateTime            product     quantity\n           <dbl>      <dbl>       <dbl> <dttm>              <chr>          <dbl>\n 1       1002961    2000253     3000047 2024-05-14 12:17:01 Golden Gat~        8\n 2       1003007    2000226     3000047 2024-05-10 23:10:10 Austin Alm~       36\n 3       1003017    2000108     3000047 2024-05-16 16:34:10 Austin Alm~       40\n 4       1003068    2000173     3000047 2024-05-02 04:31:51 Pearly Pies       28\n 5       1003103    2000075     3000047 2024-05-04 23:44:26 Pearly Pies       28\n 6       1003147    2000295     3000047 2024-05-15 16:17:06 Austin Alm~       32\n 7       1003196    2000237     3000047 2024-05-07 11:13:22 Tokyo Tidb~       40\n 8       1003329    2000272     3000047 2024-05-06 03:32:16 Outback Oa~       28\n 9       1001264    2000209     3000047 2024-05-16 17:32:28 Pearly Pies       28\n10       1001287    2000120     3000047 2024-05-15 08:41:28 Austin Alm~       40\n# i 4 more variables: unitPrice <dbl>, totalPrice <dbl>, paymentMethod <chr>,\n#   cardNumber <dbl>\n```\n\n\n:::\n:::\n\n:::\n\nYou can run any Databricks SQL, including aggregations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndaily_revenue <- dbGetQuery(con, \"\n  SELECT\n    DATE(dateTime) AS sale_date,\n    COUNT(*) AS num_transactions,\n    ROUND(SUM(totalPrice), 2) AS total_revenue,\n    ROUND(AVG(totalPrice), 2) AS avg_transaction\n  FROM samples.bakehouse.sales_transactions\n  GROUP BY DATE(dateTime)\n  ORDER BY sale_date DESC\n  LIMIT 20\n\")\n```\n:::\n\n\n::: {.callout-note title=\"Output: Daily Revenue Aggregation\" collapse=\"true\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 17 x 4\n   sale_date  num_transactions total_revenue avg_transaction\n   <date>                <dbl>         <dbl>           <dbl>\n 1 2024-05-17              106          1932            18.2\n 2 2024-05-16              203          3714            18.3\n 3 2024-05-15              192          3804            19.8\n 4 2024-05-14              218          4221            19.4\n 5 2024-05-13              229          4044            17.7\n 6 2024-05-12              202          4398            21.8\n 7 2024-05-11              206          3747            18.2\n 8 2024-05-10              181          3729            20.6\n 9 2024-05-09              219          4320            19.7\n10 2024-05-08              219          3921            17.9\n11 2024-05-07              191          3894            20.4\n12 2024-05-06              195          4500            23.1\n13 2024-05-05              181          3945            21.8\n14 2024-05-04              190          3822            20.1\n15 2024-05-03              204          4278            21.0\n16 2024-05-02              190          4074            21.4\n17 2024-05-01              207          4128            19.9\n```\n\n\n:::\n:::\n\n:::\n\n## Step 7: Get a Table with dplyr\n\nBrickster's dbplyr integration lets you write dplyr code that gets translated to SQL and executed on Databricks. The data stays on the server until you call `collect()`.\n\nCreate a lazy reference to the sales table:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsales_tbl <- tbl(con, I(\"samples.bakehouse.sales_transactions\"))\n```\n:::\n\n\n::: {.callout-note title=\"Output: tbl() Preview\" collapse=\"true\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\ni Submitting query\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nv Submitting query [874ms]\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\ni Fetching 21 rows\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nv Downloaded 21 rows [107ms]\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\ni Processing results\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nv Processing results [3ms]\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Source:   table<samples.bakehouse.sales_transactions> [?? x 10]\n# Database: DatabricksConnection\n   transactionID customerID franchiseID dateTime            product     quantity\n           <dbl>      <dbl>       <dbl> <dttm>              <chr>          <dbl>\n 1       1002961    2000253     3000047 2024-05-14 12:17:01 Golden Gat~        8\n 2       1003007    2000226     3000047 2024-05-10 23:10:10 Austin Alm~       36\n 3       1003017    2000108     3000047 2024-05-16 16:34:10 Austin Alm~       40\n 4       1003068    2000173     3000047 2024-05-02 04:31:51 Pearly Pies       28\n 5       1003103    2000075     3000047 2024-05-04 23:44:26 Pearly Pies       28\n 6       1003147    2000295     3000047 2024-05-15 16:17:06 Austin Alm~       32\n 7       1003196    2000237     3000047 2024-05-07 11:13:22 Tokyo Tidb~       40\n 8       1003329    2000272     3000047 2024-05-06 03:32:16 Outback Oa~       28\n 9       1001264    2000209     3000047 2024-05-16 17:32:28 Pearly Pies       28\n10       1001287    2000120     3000047 2024-05-15 08:41:28 Austin Alm~       40\n# i more rows\n# i 4 more variables: unitPrice <dbl>, totalPrice <dbl>, paymentMethod <chr>,\n#   cardNumber <dbl>\n```\n\n\n:::\n:::\n\n:::\n\n::: {.callout-tip title=\"The I() Wrapper\" appearance=\"simple\"}\nUse `I()` around fully-qualified table names (`catalog.schema.table`) to tell dbplyr to treat the string as a literal identifier rather than trying to parse it.\n:::\n\nNow use dplyr verbs — these translate to SQL behind the scenes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntop_products <- sales_tbl |>\n  group_by(product) |>\n  summarise(\n    total_sold = n(),\n    total_revenue = sum(totalPrice, na.rm = TRUE),\n    avg_price = mean(unitPrice, na.rm = TRUE)\n  ) |>\n  arrange(desc(total_revenue)) |>\n  head(10) |>\n  collect()\n```\n:::\n\n\n::: {.callout-note title=\"Output: Top Products\" collapse=\"true\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 4\n  product                total_sold total_revenue avg_price\n  <chr>                       <dbl>         <dbl>     <dbl>\n1 Golden Gate Ginger            586         11595         3\n2 Outback Oatmeal               561         11199         3\n3 Austin Almond Biscotti        530         11148         3\n4 Tokyo Tidbits                 583         10986         3\n5 Pearly Pies                   550         10785         3\n6 Orchard Oasis                 523         10758         3\n```\n\n\n:::\n:::\n\n:::\n\nYou can inspect the generated SQL with `show_query()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsales_tbl |>\n  group_by(product) |>\n  summarise(total_sold = n()) |>\n  arrange(desc(total_sold)) |>\n  head(5) |>\n  show_query()\n```\n:::\n\n\n## Step 8: Basic Analysis\n\nLet's do some analysis on the Bakehouse data by combining dplyr operations and bringing the results into R.\n\n### Revenue by Franchise\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfranchise_revenue <- sales_tbl |>\n  group_by(franchiseID) |>\n  summarise(\n    total_revenue = sum(totalPrice, na.rm = TRUE),\n    num_transactions = n(),\n    avg_transaction = mean(totalPrice, na.rm = TRUE)\n  ) |>\n  arrange(desc(total_revenue)) |>\n  collect()\n```\n:::\n\n\n::: {.callout-note title=\"Output: Revenue by Franchise\" collapse=\"true\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 48 x 4\n   franchiseID total_revenue num_transactions avg_transaction\n         <dbl>         <dbl>            <dbl>           <dbl>\n 1     3000046          6642               63           105. \n 2     3000047          4512               60            75.2\n 3     3000000          2790               83            33.6\n 4     3000021          2514               64            39.3\n 5     3000002          2502               76            32.9\n 6     3000033          1482               90            16.5\n 7     3000010          1479               82            18.0\n 8     3000011          1404               86            16.3\n 9     3000045          1392               82            17.0\n10     3000030          1371               77            17.8\n# i 38 more rows\n```\n\n\n:::\n:::\n\n:::\n\n### Transaction Distribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntransaction_stats <- sales_tbl |>\n  summarise(\n    total_transactions = n(),\n    min_amount = min(totalPrice, na.rm = TRUE),\n    max_amount = max(totalPrice, na.rm = TRUE),\n    avg_amount = mean(totalPrice, na.rm = TRUE)\n  ) |>\n  collect()\n```\n:::\n\n\n::: {.callout-note title=\"Output: Transaction Stats\" collapse=\"true\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 4\n  total_transactions min_amount max_amount avg_amount\n               <dbl>      <dbl>      <dbl>      <dbl>\n1               3333          3        180       19.9\n```\n\n\n:::\n:::\n\n:::\n\n### Monthly Trends\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmonthly_trends <- sales_tbl |>\n  mutate(\n    sale_month = date_trunc(\"month\", dateTime)\n  ) |>\n  group_by(sale_month) |>\n  summarise(\n    revenue = sum(totalPrice, na.rm = TRUE),\n    transactions = n()\n  ) |>\n  arrange(sale_month) |>\n  collect()\n```\n:::\n\n\n::: {.callout-note title=\"Output: Monthly Trends\" collapse=\"true\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 3\n  sale_month          revenue transactions\n  <dttm>                <dbl>        <dbl>\n1 2024-05-01 00:00:00   66471         3333\n```\n\n\n:::\n:::\n\n:::\n\n### Transaction Heatmap by Day of Week and Hour\n\nOnce data is collected into R, you can use any R visualisation library. Here we use a custom `theme_dailydatabricks()` ggplot theme with the logo watermark.\n\n::: {.callout-note title=\"theme_dailydatabricks() — Custom ggplot Theme\" collapse=\"true\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(png)\nlibrary(grid)\n\n# Load the logo for watermark\nlogo_img <- readPNG(\"Assets/logo-icon.png\")\nlogo_grob <- rasterGrob(\n  logo_img,\n  x = unit(1, \"npc\") - unit(0.4, \"cm\"),\n  y = unit(0.4, \"cm\"),\n  hjust = 1, vjust = 0,\n  width = unit(1.2, \"cm\"),\n  interpolate = TRUE\n)\n\n# Custom DailyDatabricks theme\ntheme_dailydatabricks <- function(base_size = 12) {\n  theme_minimal(base_size = base_size) %+replace%\n    theme(\n      plot.title = element_text(\n        face = \"bold\", size = rel(1.3), hjust = 0,\n        margin = margin(b = 8)\n      ),\n      plot.subtitle = element_text(\n        size = rel(0.9), colour = \"#666666\", hjust = 0,\n        margin = margin(b = 12)\n      ),\n      plot.caption = element_text(\n        size = rel(0.7), colour = \"#999999\", hjust = 1\n      ),\n      axis.title = element_text(size = rel(0.85), colour = \"#333333\"),\n      axis.text = element_text(size = rel(0.8), colour = \"#555555\"),\n      panel.grid.major = element_line(colour = \"#F0F0F0\", linewidth = 0.4),\n      panel.grid.minor = element_blank(),\n      legend.position = \"bottom\",\n      legend.title = element_text(size = rel(0.85)),\n      legend.text = element_text(size = rel(0.75)),\n      plot.margin = margin(15, 15, 15, 15)\n    )\n}\n```\n:::\n\n:::\n\nLet's build a heatmap of transaction volume across day of week and hour of day — a useful way to spot when the bakery is busiest:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pull transaction counts by day of week and hour, computed on Databricks\nheatmap_data <- sales_tbl |>\n  mutate(\n    dow = dayofweek(dateTime),\n    hour = hour(dateTime)\n  ) |>\n  group_by(dow, hour) |>\n  summarise(transactions = n(), .groups = \"drop\") |>\n  collect() |>\n  mutate(\n    day_name = factor(\n      dow,\n      levels = 1:7,\n      labels = c(\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\")\n    ),\n    hour_label = sprintf(\"%02d:00\", hour)\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\ni Submitting query\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nv Submitting query [922ms]\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\ni Fetching 168 rows\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nv Downloaded 168 rows [102ms]\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\ni Processing results\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nv Processing results [3ms]\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n```{.r .cell-code}\n# Build the heatmap\np <- ggplot(heatmap_data, aes(x = hour, y = day_name, fill = transactions)) +\n  geom_tile(colour = \"white\", linewidth = 0.6) +\n  scale_fill_gradient(\n    low = \"#FDEAEA\",\n    high = \"#CC3333\",\n    name = \"Transactions\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23, 3),\n    labels = sprintf(\"%02d:00\", seq(0, 23, 3)),\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(expand = c(0, 0)) +\n  labs(\n    title = \"Bakehouse Transaction Volume\",\n    subtitle = \"By day of week and hour of day\",\n    x = NULL,\n    y = NULL,\n    caption = \"dailydatabricks.tips\"\n  ) +\n  theme_dailydatabricks() +\n  annotation_custom(logo_grob)\n\np\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/heatmap-1.png){width=960}\n:::\n:::\n\n\n## Step 9: Write Back to a New Table (Example Only)\n\n::: {.callout-warning title=\"Not Executed\" appearance=\"simple\"}\nThe code below demonstrates how to write data back to Databricks. It is **shown but not executed** to avoid creating tables in your workspace. Adjust the catalog, schema, and table names to match your environment before running.\n:::\n\nFor small datasets (under 50,000 rows), brickster uses inline SQL `INSERT` statements:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Write the franchise revenue summary to a new table\ndbWriteTable(\n  con,\n  name = \"main.default.bakehouse_franchise_summary\",\n  value = franchise_revenue\n)\n```\n:::\n\n\nFor larger datasets, brickster stages the data as Parquet in a Unity Catalog volume before loading it. You need to specify a staging volume:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# For datasets > 50,000 rows, provide a staging volume\ndbWriteTable(\n  con,\n  name = \"main.default.bakehouse_large_analysis\",\n  value = large_result,\n  staging_volume = \"/Volumes/main/default/staging\"\n)\n```\n:::\n\n\nYou can also use SQL to create tables from query results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbExecute(con, \"\n  CREATE TABLE main.default.bakehouse_monthly_trends AS\n  SELECT\n    DATE_TRUNC('month', dateTime) AS sale_month,\n    SUM(totalPrice) AS revenue,\n    COUNT(*) AS transactions\n  FROM samples.bakehouse.sales_transactions\n  GROUP BY DATE_TRUNC('month', dateTime)\n\")\n```\n:::\n\n\n::: {.callout-tip title=\"Overwrite vs Append\" appearance=\"simple\"}\nBy default, `dbWriteTable()` will fail if the table already exists. Use `overwrite = TRUE` to replace the table, or `append = TRUE` to add rows to an existing table.\n:::\n\n## Cleanup & Cost Management\n\nDisconnect from the warehouse when you're done:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbDisconnect(con)\n```\n:::\n\n\n::: {.callout-important title=\"Stop Compute Resources\" appearance=\"simple\"}\nMake sure to **stop your SQL Warehouse** in the Databricks UI when you're finished to avoid unnecessary costs. If you started a cluster for library installation, stop that too.\n:::\n\n## References & Further Reading\n\n-   [Brickster Documentation](https://databrickslabs.github.io/brickster/)\n-   [Brickster on CRAN](https://cran.r-project.org/package=brickster)\n-   [Brickster GitHub Repository](https://github.com/databrickslabs/brickster)\n-   [Databricks SQL Statement Execution API](https://docs.databricks.com/api/workspace/statementexecution)\n-   [dbplyr Documentation](https://dbplyr.tidyverse.org/)\n-   [Unity Catalog Overview](https://docs.databricks.com/en/data-governance/unity-catalog/index.html)\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}